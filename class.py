# -*- coding: utf-8 -*-
"""class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17HM-21VmniswGZDxUm9l4Y9HpLzMPnau
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing the necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")
import os
import plotly.graph_objects as go
import plotly.express as px
import statistics
import scipy
import scipy.stats as stats
from scipy.stats import skew
from scipy.stats import kurtosis
# %matplotlib inline

import datetime
from dateutil.relativedelta import relativedelta

import warnings
warnings.filterwarnings('ignore')

class DataProcessor:
  def __init__(self, file_path):
    self.file_path = file_path
    try:
      assert self.file_path[len(self.file_path)-4:]=='.csv', 'Please provide a csv file'
      self.df = pd.read_csv(self.file_path)
    except AssertionError as e:
      return str(e)

  def load_data(self):
    # self.df = pd.read_csv(self.file_path)
    print(self.df.head())
    print('-----'*21)

    # Printing the shape of the dataframe
    print(f'The dataframe contains {self.df.shape[0]} rows and {self.df.shape[1]} columns')
    print('-----'*21)

    # Printing the columns of the dataframe
    print(self.df.columns)
    print('-----'*21)

    # Printing the basic information of the dataframe
    print(self.df.info())
    print('-----'*21)

    # Printing the statistical information of the dataframe
    print(self.df.describe())
    print('-----'*21)

  def clean_data(self):
    # Creating a dictionary to store the number of unique values of the column
    unique_dict = {}
    for i in self.df.columns:
      if i not in list(unique_dict.keys()):
        unique_dict[i]= self.df[i].nunique()
    
    # Dropping the column which creates an extra index
    drop_list = []
    for i,j in unique_dict.items():
      if j==self.df.shape[0]:
        drop_list.append(i)
    self.df.drop(drop_list, axis=1, inplace=True)

    # Imputing the null values of the discount with 0.00 as null discount means 0 discount
    self.df['Cash_Discount_Per_Case_Off_Invoice'].fillna(0.00, inplace=True)
    self.df['Cash_Discount_Per_Case_On_Invoice'].fillna(0.00, inplace=True)

    # Creating a dictionary to store the percentage of null values in each column
    null_dict = {}
    for i in self.df.columns:
      if i not in list(null_dict.keys()): 
        null_dict[i]=self.df[i].isnull().mean()*100

    # Dropping the columns which as more than 30% of null values
    drop_cols = []
    for i,j in null_dict.items():
      if j>=30.00:
        drop_cols.append(i)
    self.df.drop(drop_cols, axis=1, inplace=True)

    # Dropping the null values
    self.df.dropna(inplace=True)

  def pre_processing(self):
    a=self.df.isnull().sum().sum()
    try:
      assert a==0, 'Please clean the data first'
      # Splitting the 'Shipment_week' column to extract further information from it  
      self.df['new_shipment_week'] = self.df['Shipment_week'].apply(lambda x:x.split('-'))
      
      # Function to extract the 1st date of every week from the 'Shipment_week' column
      def convert_date(lst):
        year = int(lst[0])
        week = int(lst[1])
        date = datetime.date(year, 1, 1)
        res = date + relativedelta(weeks = +week)
        return res
      
      # Applying the above function to extract the 1st date of every week
      self.df['shipment_date'] = self.df['new_shipment_week'].apply(convert_date)

      # Converting the shipment date to a datetime object
      self.df['shipment_date'] = pd.to_datetime(self.df['shipment_date'])

      # Extracting the year from the date colunm
      self.df['shipment_year'] = self.df['shipment_date'].dt.year
      self.df['shipment_year'] = self.df['shipment_year'].astype(str)

      # Extracting the month name from the date column
      self.df['shipment_month'] = self.df['shipment_date'].dt.month_name()

      # Creating a new column for total discount
      self.df['total_discount']=self.df['Cash_Discount_Per_Case_On_Invoice']+self.df['Cash_Discount_Per_Case_Off_Invoice']

      # Creating a combined parameter 'RF_Score' with recency and frequency
      self.df['Recency Rank'] = self.df['Recency'].rank(method='min', ascending=False)
      self.df['Frequency Rank'] = self.df['Frequency'].rank(method='min', ascending=True)
      self.df['RF_Score'] = (self.df['Recency Rank'] + self.df['Frequency Rank'])/2

      # Dropping the columns which are not useful for the analysis further
      cols = ['Shipment_week', 'Snapshot_week', 'Cash_Discount_Per_Case_Off_Invoice','Cash_Discount_Per_Case_On_Invoice',
              'new_shipment_week','shipment_date','Recency Rank','Frequency Rank']
      self.df.drop(cols, axis=1, inplace=True)

      # Converting the datatype of the required columns
      self.df['Customer_ID'] = self.df['Customer_ID'].astype(str)

      # Segregating the features into numerical and categorical
      self.cat_vars = self.df.select_dtypes(include=['object']).columns.to_list()
      self.num_vars = self.df.select_dtypes(include=['int64', 'float64']).columns.to_list()

      # Creating the dataframes for the numerical and categorical variables
      self.num_df = self.df[self.num_vars]
      self.cat_df = self.df[self.cat_vars]

      # Calculating the standard deviation and mean for the numerical columns dataframe
      std_dev = self.num_df.std()
      mean = self.num_df.mean()
      
      # Capping the outliers for the numerical data
      max_val = mean + 3*std_dev
      min_val = mean - 3*std_dev
      self.num_df= self.num_df.clip(min_val, max_val, axis=1)

      # Creating a final dataframe by merging the numerical columns and categorical columns dataframe
      self.df2 = pd.merge(self.num_df, self.cat_df, left_index=True, right_index=True)

    except AssertionError as e:
      return str(e)

  def univariate_analysis(self):
    try:
      assert self.df.shape[0]*100/self.df.shape[0]>50, 'More than 50% of the data has been cleaned and thus data information has been lost'
      # Removing the column names which are not required
      self.cat_vars.remove('Key')
      self.cat_vars.remove('Customer_ID')
      
      # Plotting the countplot for all the categorical variables
      for i in self.cat_vars:
        print(f'{i}:')
        print(self.df2[i].value_counts(normalize=True)*100)
        print('-----'*21)
        plt.figure(figsize=(8, 5))
        figure = sns.countplot(x=self.df2[i])
        plt.xticks(rotation=45)
        plt.show(figure)

      # Printing the summary statistics and plotting the density plot and boxplot for the numerical variables
      for i in self.num_vars:
        print(f'{i}:')
        print(f'Mean: {self.df2[i].mean()}')
        print(f'Median: {self.df2[i].median()}')
        print(f'Mode: {statistics.mode(self.df2[i])}')
        print(f'Standard Deviation: {self.df2[i].std()}')
        print(f'Variance: {statistics.variance(self.df2[i])}')
        print(f'Range: {max(self.df2[i])-min(self.df2[i])}')
        q3,q1 = np.percentile(self.df2[i], [75 ,25])
        iqr = q3-q1
        print(f'Interquartile Range: {iqr}')
        skew = stats.skew(self.df2[i], axis=0, bias=True)
        if skew==0:
            print(f'Skewness {skew} --> (Normal Distribution)')
        elif skew>0:
            print(f'Skewness {skew} --> (Right Skewed)')
        else:
            print(f'Skewness {skew} --> (Left Skewed)')

        kurtosis = stats.kurtosis(self.df2[i], axis=0, bias=True)
        if kurtosis==3:
            print(f'Kurtosis {kurtosis} --> (Normal Distribution)')
        if kurtosis<3:
            print(f'Kurtosis {kurtosis} --> (Playkurtic)')
        else:
            print(f'Kurtosis {kurtosis} --> (Leptokurtic)')
        
        plt.figure(figsize=(15, 5))
        plt.subplot(1, 2, 1)
        figure = sns.distplot(x=self.df2[i])
        figure.set(title = f'Density Distribution of {i}')
        figure.set(xlabel=i, ylabel='Distribution')

        plt.subplot(1, 2, 2)
        figure = sns.boxplot(x=self.df2[i])
        figure.set(title = f'Distribution of {i}')
        figure.set(xlabel = i)
        plt.show()
        print('-----'*21)
    except AssertionError as e:
      return str(e)

  def bivariate_analysis(self):
    # Plotting a correlation plot for all the numerical variables
    plt.figure(figsize=(8,5))
    sns.heatmap(self.df2.corr(), annot=True, cmap='Spectral_r')

    # Plotting the sum of numerical columns against the shipment year to get the numbers over the years
    for i in self.num_df.columns:
      ddf = pd.DataFrame(self.df2.groupby('shipment_year')[i].sum().reset_index())
      plt.figure(figsize=(8, 5))
      plt.xticks(rotation=45)
      sns.barplot(data=ddf, x='shipment_year', y=i, order=['2019', '2020', '2021', '2022'])

    # Plotting the sum of numerical columns against the shipment month to get the numbers over the years
    for i in self.num_df.columns:
      ddf = pd.DataFrame(self.df2.groupby('shipment_month')[i].sum().reset_index())
      plt.figure(figsize=(8,5))
      plt.xticks(rotation=45)
      sns.barplot(data=ddf, x='shipment_month', y=i, order=['January','February','March','April','May','June','July',
                                                            'August','September','October','November','December'])

    # Plotting the sum of numerical columns against the category to get the popular category
    for i in self.num_df.columns:
      ddf = pd.DataFrame(self.df2.groupby('Category')[i].sum().reset_index())
      plt.figure(figsize=(8,5))
      plt.xticks(rotation=45)
      sns.barplot(data=ddf, x='Category', y=i)
    
    # Plotting the scatter plot against all the numerical variables to get the relation between all the numerical variables 
    for i in self.num_df.columns:
      for j in self.num_df.columns:
        if i!=j:
          figure = sns.jointplot(x=i, y=j, data=self.df2, kind="scatter", alpha=0.5)
          plt.show(figure)
  
  def trivariate_analysis(self):
    for i in self.num_df.columns:
      ddf = pd.DataFrame(self.df2.groupby(['shipment_year', 'Category'])[i].sum().reset_index())
      plt.figure(figsize=(8,5))
      plt.xticks(rotation=45)
      sns.barplot(data=ddf, x='shipment_year', y=i, hue='Category')

    for i in self.num_df.columns:
      ddf = pd.DataFrame(self.df2.groupby(['shipment_month', 'Category'])[i].sum().reset_index())
      plt.figure(figsize=(8,5))
      plt.xticks(rotation=45)
      sns.barplot(data=ddf, x='shipment_month', y=i, hue='Category', order=['January','February','March','April','May','June','July',
                                                            'August','September','October','November','December'])
  
    # Top 5 products delivered of all time
    print('Top 5 products delivered of all time')
    print(self.df2.groupby(['Key','Category']).agg({'Actuals':"sum",'total_discount':"sum"}).reset_index().sort_values('Actuals', ascending=False).head())
    print('-----'*21)
    # Top 5 products which offered the maximum discount over the years
    print('Top 5 products which offered the maximum discount over the years')
    print(self.df2.groupby(['Key','Category']).agg({'Actuals':"sum",'total_discount':"sum"}).reset_index().sort_values('total_discount', ascending=False).head())
    print('-----'*21)
    # Top 5 customers who were the most loyal in terms of their frequency and recency
    print('Top 5 customers who were the most loyal in terms of their frequency and recency')
    print(self.df2.groupby(['Customer_ID']).agg({'Actuals':"sum",'total_discount':"sum",'RF_Score':"mean"}).reset_index().sort_values('RF_Score', ascending=False).head())
    print('-----'*21)
    # Top 5 customers who got the most orders delivered
    print('Top 5 customers who got the most orders delivered')
    print(self.df2.groupby(['Customer_ID']).agg({'Actuals':"sum",'total_discount':"sum",'RF_Score':"mean"}).reset_index().sort_values('Actuals', ascending=False).head())
    print('-----'*21)
    # Top 5 customers who availed the highest discounts
    print('Top 5 customers who availed the highest discounts')
    print(self.df2.groupby(['Customer_ID']).agg({'Actuals':"sum",'total_discount':"sum",'RF_Score':"mean"}).reset_index().sort_values('total_discount', ascending=False).head())